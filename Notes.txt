    In general it appeared that sage did the best across most datasets
most notably it seems that sage was able to essentially approximate a
NN on the tests

    For most the graphs, it showed that given more feature data a NN
could always at least match the model. However with the transformer 
there was an edge information cut off after which no amount of feature information
can ensure the NN outperforms the model. This implies that edge information
is more important after a point than feature data.

    For the GCN we can see that the models in general have a hard time
dealing with heterophily(seen as the blue moving right). This is further
exasturbated by the additional classes added. Although heterophily can 
provide similar information it appears that the model needs more heterophilous
data than homogenous data to perform similarly. The SAGE and transformer
appear to overcome this limitation at least initially
    In the heterophilous case, the GCN and GAT suffer from the additional
classes far more than the SAGE and GAT, most notably in three classes. The 
SAGE and the transformer suffer a accuracy decrease, however they both appear
to be able to extract a similar amount of information from both the heterophilous
and homophilous cases

    The GAT suffers the most from the addition of classes, then GCN, the transformer

    what the regions mean:
        if there is a flat side, that means that one type of information is significantly
    more important than the other, this is due to the fact that we maintain the cutoff even
    after increasing the value of one.

    From this we can see that the transformer values the edge data much more than
the feature data up to a specific point
    The message passing models(GCN,SAGE,GAT) have a goldilocks zone with less 
feature and edge data(its closer to the origin). So they might be useful to use 
if we know we have a lower degree of separability.

    The lines where the plot switched to favoring a specific model(eg switches regions)
could be interpretted as the area where the models are equivilant

Things that could be good to know
    - comparison across models
    - comparison across different number of classes
    - comparison to DC 
    - just a general interpretation
        -

    The GCN towards the origin performs badly just due to low amounts of information.
As we increase edge information(with 0 features) we see that the model doesn't improve, this is due to the fact that it 
needs some amount of feature information. The model is able improve if we keep 0 edges and more features
just due to the implicit self loops that are present in the model. What is cool is that the 
red regions on either side are not linear or straight lines. If they were linear we would know that
there is a positive trade off between edge information and feature information. However it is curved
which shows that the edge and feature information can work together to perform better
than the sum of their parts.
    Analyzing the regions where the GCN performed better can lead to some insights. If
we solely compare the NN we see that the GCN performs better when there is limited feature
information and lots of edge information. On the opposing side the Spectral comparison implies
that the GCN performs better under low amounts of edge information but high amounts of 
feature information. Combining these two regions we obtain a "goldilocks" region where the model
is most valuable.
    Additionally the graph is not symmtric but slightly moved to the left. This can be seen more
clearly if we only focus on the blue part. This shows that at least initially the GCN has more trouble
with heterophily, even though eventually it is able to extract the same amount of information

    The comparisons to the NN across different models change. But it appears that the regions comparing
the spectral clustering stay the same for all of the message passing models. In some sense this shows that 
towards the tail of their distribution the models all essentially performed identically. The transformer
did have a similar tail so its region is way different, it is larger. Showing that the model needs more edge
information than its message passing counterparts.
    We see a lot of models doing more with more edge information and less feature information, do we see
the opposite at all?
    Additionally by studying where the models plateu we can see where the edge information/ feature
information stop helping the models perform

    Comparing the GCN to the GAT we see that the GAT is able to utilize edge information much quicker and outperform
a NN with much less edge information. It performs better all around on the 2_class case. The SAGE performed even better
than the GAT and appeared to be able to fully utilize feature information like a NN given there was no edge info. The
in some sense was also able to work like a NN, yet it needed more feature information. Also strangely, there appears to 
be a minimum amount of edge information that needs to be present in order for the transformer to outperform the NN

    NN and Spectral clustering across classes. Both NN and Spectral clustering suffer an accuracy loss when increasing the number
of classes. The spectral clustering suffers the most
    The GCN also sees an accuracy loss, but in addition to that it sees a shift in the distribution of the accuracy. As more
edges are added, heterophily becomes more and more harmful as can be seen by the distribution shifting left
    The GAT appears to suffer even more from the additional classes, especially from heterophily. While the GCN seems to be able,
given enough feature information, to acheive strong recovery, the GAT is unable to do that. The GCN and GAT perform similarly for
for the homophilous case.
    The SAGE doesn't appear to suffer as badly as the GAT or GCN, often it seems to try and approximate a NN. Given this it performs
a lot better than the other two in the heterophilous case and seems to just ignore the edge information. Additionally, the distribution
appears to spread out and flatten, likely indicating edges are less and less useful. Although the distribution of the sage does not shift
to the left as it does with the other two, This could indicate that heterophily is not as harmful for this model.
    The Transformer really just appears to be similar to the SAGE method perhaps doing slightly worse in utilizing edge and feature data

    When Comparing DC to the regular GCN the DC just appears to do slightly worse when compared to the NN and the Spectral methods
    Same for GAT
    The Appears to be the same for DC and non DC with the exception that spectral methods have a larger region where they outperform the 
SAGE model.

Increasing the Node Size by 10 and 20 had virtually no effect